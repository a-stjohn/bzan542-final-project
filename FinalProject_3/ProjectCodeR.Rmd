---
title: "Project542"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r cars}
#import data then fit a random forest with train 
library(caret)
library(parallel)
library(doParallel)
library(vip)
library(rpart)
library(rpart.plot)
library(regclass)
library(naivebayes)
library(ROSE)

#split train and test data
load("542.RData")

table(data1$GradeCat) #pretty imbalanced

265/(265+130) # naive accuracy

overdata1=ovun.sample(GradeCat~.,data = DATA,method = "over",N=530)$data
table(overdata1$GradeCat) #perfect 50/50 split

nrows<-sample(nrow(overdata1),size = 0.8*nrow(overdata1))
CLTRAIN<-overdata1[nrows,]
CLTEST<-overdata1[-nrows,]
length(colnames(CLTRAIN)) #168 possible x vars 
#make clusters for cores
cluster <- makeCluster(detectCores() - 1) #16 cores 
cluster
registerDoParallel(cluster) 
fitControl <- trainControl(method="cv",number=4,classProbs = TRUE, allowParallel = T) 

#model parameters 
#nnet parameters
nnetGrid <- expand.grid(size=1:4,decay=10^( seq(-5,-2,length=10) ) )
#xgb grid 
xgboostGrid <- expand.grid(eta=0.1,nrounds=c(100,200,500,1000,1500),
max_depth=5,min_child_weight=1,gamma=0,colsample_bytree=0.8,subsample=0.8)
#svm parameters
svmPolyGrid <- expand.grid(degree=2:3, scale=10^seq(-4,-1,by=1), C=2^(2:4) )
#knn parameters
knnGrid <- expand.grid(k=1:40)
#rf parameters
forestGrid <- expand.grid(mtry=seq(1,25,by=1))
ntrees<-seq(500,1000, by = 200)
#gbm parameters
gbmGrid <- expand.grid(n.trees=c(200,500),
interaction.depth=1:3,
shrinkage=c(.05,.1),
n.minobsinnode=c(2,5,10))




#Decision Tree Parameters
treeGrid <- expand.grid(cp=10^seq(-5,-1,length=25))
TREE <- train(GradeCat~.,data=CLTRAIN,method="rpart", tuneGrid=treeGrid,trControl=fitControl, preProc = c("center", "scale"))
TREE$bestTune
TREE$results[rownames(TREE$bestTune),][1]
#postResample(predict(TREE,newdata=CLTEST),CLTEST$GradeCat)
TREE = rpart(GradeCat~.,data = CLTRAIN,cp = 0.06812921)
visualize_model(TREE)

#rf mods per ntree lists, (caret dosent or used to not let you put the ntrees in the grid so you have to loop through them)
rflist<-list() #this will hold the rf mods
bestrfmods<-list()

#lists for all models 
modslist<-list() #keeps track of the kth best model object
bestmodslist<-list() #keeps track of the kth best model parameters

#models to loop through 
models<-c("nnet","xgb","svm","knn","gbm","rf")

for (k in 1:length(models)) {
  if(models[k]=="rf"){
    for (i in 1:length(ntrees)) {
      rflist[[i]]<-train(GradeCat~.,data=CLTRAIN,method="rf",preProc=c("center","scale"), trControl=fitControl,tuneGrid=forestGrid, importances=TRUE, ntree=ntrees[i]) 
      bestrfmods[[i]]<-rflist[[i]]$results[rownames(rflist[[i]]$bestTune),]
    }
    modslist[[k]]<-rflist[[which(unlist(bestrfmods)[which(names(unlist(bestrfmods))=="Accuracy")]==max(unlist(bestrfmods)[which(names(unlist(bestrfmods))=="Accuracy")]))]]
    bestmodslist[[k]]<-bestrfmods[[which(unlist(bestrfmods)[which(names(unlist(bestrfmods))=="Accuracy")]==max(unlist(bestrfmods)[which(names(unlist(bestrfmods))=="Accuracy")]))]]
  }
  if(models[k]=="gbm"){
    GBM<-train(GradeCat~.,data=CLTRAIN,method="gbm",trControl=fitControl,
tuneGrid=gbmGrid,preProc=c("center","scale"),verbose=FALSE)
    modslist[[k]]<-GBM
    bestmodslist[[k]]<-GBM$results[rownames(GBM$bestTune),]
  }
  if(models[k]=="knn"){
    KNN <- train(GradeCat~.,data=CLTRAIN,method='knn', tuneGrid=knnGrid,
trControl=fitControl, preProc = c("center", "scale"))
    modslist[[k]]<-KNN
    bestmodslist[[k]]<-KNN$results[rownames(KNN$bestTune),]
  }
  if(models[k]=="svm"){
    SVMpoly <- train(GradeCat~.,data=CLTRAIN,method='svmPoly',trControl=fitControl,tuneGrid=svmPolyGrid,preProc = c("center", "scale"))
    modslist[[k]]<-SVMpoly
    bestmodslist[[k]]<-SVMpoly$results[rownames(SVMpoly$bestTune),]
  }
  if(models[k]=="xgb"){
    XTREME<-train(GradeCat~.,data=CLTRAIN,method="xgbTree",trControl=fitControl,tuneGrid=xgboostGrid,preProc = c("center", "scale"),verbose=FALSE)
    modslist[[k]]<-XTREME
    bestmodslist[[k]]<-XTREME$results[rownames(XTREME$bestTune),]
  }
  if(models[k]=="nnet"){
    NNET <- train(GradeCat~.,data=CLTRAIN,method='nnet',trControl=fitControl,tuneGrid=nnetGrid,trace=FALSE,linout=FALSE,preProc = c("center", "scale"))
    modslist[[k]]<-NNET
    bestmodslist[[k]]<-NNET$results[rownames(NNET$bestTune),]
  }
}

bestmodslist[[6]]
bestmodslist
length(bestmodslist)
length(modslist)
modslist[[1]]
predict(modslist[[1]],CLTEST) #gets predictions

modslist
vip(modslist[[6]],15)
best




#after determine model canidates loop through choose nmods 2 and get two way ensemble models 






```

#shiny app proto 
-tab 1 visualizations, make 3 inputs x, y, and color by vars??
-tab 2 traditional models (lm log reg anova) to predict the response??
-ml models?? need to put above code in a reactive object for shiny 
-recomenaions??
-clustering?? and interactive visualization of clusters where user can choose number of clusters and the vars to plot to visualize the obs in the cluster 

```{r pressure, echo=FALSE}



```

